{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import trange\n",
    "\n",
    "from src.utils.dt import DecisionTransformer, DT_Trainer, SequenceDataset\n",
    "from src.utils.utils import count_parameters, eval_goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps for 1 look: 5566\n"
     ]
    }
   ],
   "source": [
    "SEED = 911\n",
    "\n",
    "# env\n",
    "SIZE = 9\n",
    "N_ACTIONS = 5\n",
    "REWARD_NUNIQUE = 2\n",
    "\n",
    "# dataset\n",
    "HISTORY_LEN = 800_000  # there is no `learning` histories after this step\n",
    "\n",
    "# dataloader\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# transformer\n",
    "SEQ_LEN = 256\n",
    "EPISODE_LEN = 20\n",
    "TIME_REL = True\n",
    "EMBEDDING_DIM = 64\n",
    "NUM_LAYERS = 4\n",
    "NUM_HEADS = 4\n",
    "ATTENTION_DROPOUT = 0.5\n",
    "RESIDUAL_DROPOUT = 0.1\n",
    "EMBEDDING_DROPOUT = 0\n",
    "\n",
    "# adam optimizer\n",
    "MAX_LR = 3e-4\n",
    "BETA1 = 0.9\n",
    "BETA2 = 0.99\n",
    "GRAD_CLIP_NORM = 1\n",
    "\n",
    "# cosine decay scheduler\n",
    "MIN_LR = 2e-6\n",
    "NUM_STEPS1 = int(57 * 800_000 / BATCH_SIZE / SEQ_LEN)\n",
    "print(\"steps for 1 look:\", NUM_STEPS1)\n",
    "NUM_STEPS = 4 * NUM_STEPS1\n",
    "\n",
    "EVAL_STEP = 64\n",
    "EVAL_LEN = 256\n",
    "TEST_TASKS_PATH = \"src/test_tasks.txt\"\n",
    "HISTORY_PATH = \"src/histories\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of trainable parameters:  1132101\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTransformer(\n",
    "    state_dim=SIZE * SIZE,\n",
    "    action_dim=N_ACTIONS,\n",
    "    reward_nunique=REWARD_NUNIQUE,\n",
    "    seq_len=SEQ_LEN,\n",
    "    episode_len=EPISODE_LEN,\n",
    "    time_rel=TIME_REL,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    attention_dropout=ATTENTION_DROPOUT,\n",
    "    residual_dropout=RESIDUAL_DROPOUT,\n",
    "    embedding_dropout=EMBEDDING_DROPOUT,\n",
    ")\n",
    "\n",
    "print(\"number of trainable parameters: \", count_parameters(dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SequenceDataset(\n",
    "    history_path=HISTORY_PATH,\n",
    "    history_len=HISTORY_LEN,\n",
    "    seq_len=SEQ_LEN,\n",
    "    time_rel=TIME_REL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = iter(\n",
    "    DataLoader(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        pin_memory=False,\n",
    "        num_workers=1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(dt.parameters(), lr=MAX_LR, betas=(BETA1, BETA2))\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    opt, T_max=NUM_STEPS, eta_min=MIN_LR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = DT_Trainer(dt, opt, scheduler, F.cross_entropy, GRAD_CLIP_NORM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(f\"src/logs/dt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tasks = np.loadtxt(TEST_TASKS_PATH, delimiter=\",\", dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in trange(NUM_STEPS):\n",
    "    batch = next(trainloader)\n",
    "    data = trainer.step(batch)\n",
    "\n",
    "    for stat_name in data:\n",
    "        writer.add_scalar(f\"train/{stat_name}\", data[stat_name], step)\n",
    "\n",
    "    if step % EVAL_STEP == 0:\n",
    "        with torch.no_grad():\n",
    "            test_goal = random.choice(test_tasks)\n",
    "            print(\"eval_goal:\", test_goal)\n",
    "            eval_goal(\n",
    "                dt=dt,\n",
    "                length=EVAL_LEN,\n",
    "                time_rel=TIME_REL,\n",
    "                goal=test_goal,\n",
    "                size=SIZE,\n",
    "                episode_length=EPISODE_LEN,\n",
    "                mean_ep=10,\n",
    "                writer=writer,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dt, f\"untrained_dt_E{EMBEDDING_DIM}_L{NUM_LAYERS}_H{NUM_HEADS}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
